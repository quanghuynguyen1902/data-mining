{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Su dung thu vien VnCoreNLP de tokenize word\n",
    "    link github: https://github.com/vncorenlp/VnCoreNLP\n",
    "'''\n",
    "#vncorenlp -Xmx2g \"/home/quanghuy/Desktop/data_mining/VnCoreNLP/VnCoreNLP-1.1.jar\" -p 9000 -a \"wseg,pos,ner,parse\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = [line.rstrip('\\n') for line in open('../data/train/data.txt')]\n",
    "#read stop word\n",
    "stop_words = [line.rstrip('\\n') for line in open('../data/stopword/stop_words.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__Du_lich\tTheo hành trình tour du lịch Mỹ - Bờ Đông, du khách của Lữ hành Saigontourist sẽ đến New York - giấc mơ được gọi tên của hàng triệu người - chinh phục du khách bằng tượng đài Nữ thần Tự do duyên dáng trên vịnh Hudson, trung tâm Manhattan - trụ sở tài chính lớn nhất thế giới với phố Wall, khu Rockeffeler...Và thủ đô Washington, dạo bước trên National Mall, du khách sẽ bị choáng ngợp bởi các công trình kỳ vĩ nơi đây. Từ điện Capitol, tượng đài Lincoln, đài tưởng niệm Washington... cho đến Nhà Trắng, mỗi công trình đều là một tuyệt tác kiến trúc được phối cảnh hài hòa khiến cho Washington DC từ lâu đã được ghi nhận là một trong những thành phố đẹp nhất Hợp chủng quốc Hoa Kỳ. Đặc biệt hơn khi đi du lịch Mỹ, đến bất kỳ thành phố nào vào bất cứ thời gian nào, du khách cũng có thể trải nghiệm thú vui mua sắm bất tận…  http://www.dulichtet.com/nuoc-ngoai/4253/\n"
     ]
    }
   ],
   "source": [
    "print(data[0]) # check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(len(data)) # check length data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " label nha dat cho thuê nhà ở tầng khép kín nguyễn khoái có bếp riêng đầy đủ tủ lạnh lò vi sóng bếp gas có máy giặt sân phơi điện nước dân dụng chia đầu người giowf giấc thoải mái đóng tiền tháng lần đặt cọc tháng tr tháng liên hệ inb hoặc miễn trung gian\n"
     ]
    }
   ],
   "source": [
    "def clean_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Zàáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹýđ]+', ' ', text)\n",
    "    return text\n",
    "print(clean_data(data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    text_removed = []\n",
    "    for i in range(len(text)):\n",
    "        if text[i] not in stop_words:\n",
    "            text_removed.append(text[i])\n",
    "    return text_removed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_train_Data(data):\n",
    "    label = []\n",
    "    content = []\n",
    "    for i in data:\n",
    "        t = i.split(None, 1)\n",
    "        t[0] = t[0].replace('__label__', '')\n",
    "        t[1] = clean_data(t[1])\n",
    "        label.append(t[0])\n",
    "        content.append(t[1])\n",
    "    return label, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, content = processing_train_Data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_and_removeSW(content):\n",
    "    content_save = []\n",
    "    annotator = VnCoreNLP(address=\"http://127.0.0.1\", port=9000) \n",
    "\n",
    "    for i in range(len(content)):\n",
    "        tokenized = annotator.tokenize(content[i])\n",
    "        tokenized_removed = remove_stopword(tokenized[0])\n",
    "        content_save.append(\" \".join(tokenized_removed))\n",
    "    return content_save\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = tokenized_and_removeSW(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data which tokenized and removed stop words\n",
    "dictt = {'content': contents, 'label':label}\n",
    "df = pd.DataFrame(dictt) \n",
    "df.to_csv('../data/train/data_removedStopWord.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ preprocessing data for test ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "test_data = [line.rstrip('\\n') for line in open('data.txt')]\n",
    "#read stop word\n",
    "stop_words = [line.rstrip('\\n') for line in open('/data/stop/word/stop_words.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_test_Data(data):\n",
    "    content = []\n",
    "    for i in data:\n",
    "        t = clean_data()\n",
    "        content.append(t)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = processing_test_Data(test_data)\n",
    "test_contents = tokenized_and_removeSW(test_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save test data which tokenized and removed stop words\n",
    "dictt = {'content': test_contents}\n",
    "df = pd.DataFrame(dictt) \n",
    "df.to_csv('test_data_removedStopWord.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
